# ğŸ§  A Theoreticianâ€™s Guide to Experimental Algorithm Design and Debugging  
### _(Summary and Practical Checklist based on David S. Johnsonâ€™s â€œA Theoreticianâ€™s Guide to the Experimental Analysis of Algorithmsâ€)_

---

## ğŸ¯ Purpose

This guide summarizes David S. Johnsonâ€™s lessons for conducting **scientifically rigorous algorithmic experiments**, focusing on **code optimization, debugging, reproducibility, and experimental design**.  
It translates his ten principles, pitfalls, and pet peeves into a practical programming and algorithm-improvement guide.

---

## âš™ï¸ 1. Core Philosophy

Experimental algorithmics is a **science**, not a coding contest.  
It seeks **understanding** â€” not just performance numbers.

- Implement carefully.  
- Measure systematically.  
- Explain results objectively.  
- Ensure reproducibility and comparability.  

> â€œImplementation is the easy part. The hard part is using that implementation to produce meaningful and valuable research results.â€ â€” D.S. Johnson

---

## ğŸ§© 2. Principles of Good Experimental Algorithmics

### **Principle 1 â€” Perform Newsworthy Experiments**
Work on meaningful problems and algorithms that could matter in practice.  
Avoid trivial or dominated ideas unless the insights are new.

#### ğŸ”¥ Pitfalls
1. **Dominated algorithms:** Testing obviously inferior algorithms.  
2. **Wrong questions:** Running full experiments before knowing what to measure.  
3. **Endless experimentation:** Never-ending tuning with no clear goal.  
4. **Losing focus:** Studying random instance behavior instead of algorithms.

#### ğŸ’¡ Suggestions
- **Think before you compute.**  
- Use **exploratory experiments** to discover good questions.  
- **Iterate:** prototype â†’ analyze â†’ refine â†’ conclude.

---

### **Principle 2 â€” Tie Your Work to the Literature**
- Study and understand prior results fully.  
- Compare directly to previous experiments.  
- Reproduce old baselines when possible.

#### ğŸ”¥ Pet Peeve
- Authors who cite but clearly havenâ€™t read key references.  

---

### **Principle 3 â€” Use Instance Testbeds That Support General Conclusions**
Mix structured random instances (for scalability) with real-world datasets (for realism).  
Use parameterized generators to control structure and difficulty.

#### ğŸ”¥ Pet Peeves
2. **Unstructured random instances:** Unrealistic and misleading.  
3. **Millisecond testbeds:** Problems that finish too quickly to measure meaningfully.  
4. **Already-solved testbeds:** Small or trivial datasets where optima are known.

---

### **Principle 4 â€” Use Efficient and Effective Experimental Designs**

#### ğŸ’¡ Suggestions
3. **Variance reduction:** Test all algorithms on the same random instances.  
4. **Bootstrapping:** Efficiently estimate â€œbest-of-kâ€ run behavior.  
5. **Self-documenting programs:** Log machine, compiler, version, parameters, seeds, and date in every run.

---

### **Principle 5 â€” Use Reasonably Efficient Implementations**
Efficiency matters: poor implementations waste time and distort insight.

#### ğŸ”¥ Pet Peeves
5. **â€œWe didnâ€™t have time to optimizeâ€** â€” not a valid justification.  

#### âš ï¸ Pitfall
5. **Over-tuning code:** Donâ€™t obsess over tiny constant-factor improvements.  

#### âœ… Tips
- Profile early; focus on bottlenecks.  
- Choose efficient data structures for core operations.  
- Avoid unnecessary theoretical micro-optimizations.

---

### **Principle 6 â€” Ensure Reproducibility**
Make experiments repeatable by you and others.

#### ğŸ”¥ Pet Peeves
6. **Code doesnâ€™t match the paper.**  
7. **Vague performance standards.**  
8. **Time-based stopping criteria** (e.g., â€œrun for 1 hourâ€).  
9. **Stopping when reaching known optima.**  
10. **Hand-tuned parameters without description.**  
11. **Single-run studies.**  
12. **Reporting only best results.**

#### âœ… Best Practices
- Define clear stopping conditions (e.g., iterations or operation counts).  
- Report averages, variances, and confidence intervals.  
- Include parameter tuning time in total runtime.  
- Archive and publish code, seeds, and test instances.

---

### **Principle 7 â€” Ensure Comparability**
Enable future researchers to compare their results with yours.

#### ğŸ”¥ Pet Peeves
13. **Uncalibrated machines:** Missing details about hardware or compilers.  
14. **Lost testbeds:** Instances or generators not shared.

#### âš ï¸ Pitfall
6. **Lost code or data:** Poor version control or no backups.

#### ğŸ’¡ Suggestion
6. Use benchmark codes (e.g., DIMACS or LINPACK) for machine calibration and normalization.

---

### **Principle 8 â€” Report the Full Story**
Be transparent and honest about your results.

#### ğŸ”¥ Pet Peeves
15. **False precision:** Excess digits imply false accuracy.  
16. **Unremarked anomalies:** Ignoring surprising outcomes.  
17. **Ex post facto stopping:** Reporting time until â€œbest found.â€  
18. **Ignoring total runtime:** Always report complete execution times.

---

### **Principles 9 & 10 â€” Draw Justified Conclusions & Present Data Informatively**
- Explain *why* algorithms differ in performance.  
- Use visual data summaries (boxplots, scaling plots).  
- Include anomalies and failures â€” they reveal deeper truths.  
- Support all conclusions with quantitative evidence.

---

## ğŸ§ª 3. Debugging and Quality Control

### âœ… Debugging Checklist
- Validate correctness on small, known test cases.  
- Profile code early; fix bottlenecks before large-scale runs.  
- Track all parameter values, seeds, and compiler settings.  
- Examine anomalies â€” they often reveal subtle bugs.  
- Compare your implementation against trusted baselines.

### âš ï¸ Debugging Pitfalls
- Inconsistent or undocumented random seeds.  
- Mixing code versions without tracking.  
- Misinterpreting anomalies as â€œnoise.â€  
- Deleting or overwriting original datasets.  

---

## ğŸš€ 4. Experimental Algorithm Workflow

| Step | Goal | Key Practice |
|------|------|--------------|
| 1 | **Plan** | Define meaningful questions; review literature. |
| 2 | **Prototype** | Implement baseline and confirm correctness. |
| 3 | **Explore** | Run small tests; find anomalies and key variables. |
| 4 | **Optimize** | Profile and fix performance bottlenecks. |
| 5 | **Design** | Choose varied and realistic testbeds. |
| 6 | **Measure** | Collect time, memory, and quality metrics. |
| 7 | **Replicate** | Multiple runs; apply variance control. |
| 8 | **Document** | Log all conditions and configurations. |
| 9 | **Analyze** | Explain results, not just report numbers. |
| 10 | **Report** | Publish the full, honest story with supporting data. |

---

## ğŸ“˜ 5. Key Takeaways

- **Think before you compute.**  
- **Efficiency and rigor** are equally essential.  
- **Reproducibility** defines credibility.  
- **Anomalies** are gold â€” not garbage.  
- **Transparency beats triumph:** show both strengths and weaknesses.  
- Good experimental algorithmics balances **theoryâ€™s rigor** with **engineeringâ€™s realism**.

---

### ğŸ Final Quote

> â€œThink before you compute. Then compute carefully, explain fully, and let your code â€” not your excuses â€” speak for your algorithm.â€  
> â€” David S. Johnson

---

# ğŸ§© Algorithm Debugging & Optimization Quick Checklist

A one-page practical summary for experimenters and developers.

---

## ğŸ›  Implementation
- [ ] Verify correctness with known instances.
- [ ] Use profiling tools to locate bottlenecks early.
- [ ] Use efficient, standard data structures.
- [ ] Avoid untested â€œspeed hacksâ€ that obscure correctness.

## ğŸ§ª Experiment Design
- [ ] Plan goals and metrics before coding.
- [ ] Use the same random instances across algorithms.
- [ ] Calibrate machine performance with benchmarks.
- [ ] Define algorithmic (not time-based) stopping conditions.
- [ ] Use statistically significant sample sizes.

## ğŸ§¾ Documentation
- [ ] Output algorithm name, version, date, machine, parameters, and seeds.  
- [ ] Log intermediate data and anomalies.  
- [ ] Archive all code, data, and logs with clear version control.

## ğŸ§  Analysis
- [ ] Report averages **and** variability.  
- [ ] Explain *why* one algorithm outperforms another.  
- [ ] Include negative and anomalous results.  
- [ ] Avoid overprecision; round to meaningful digits.  

## ğŸ” Reproducibility
- [ ] Make source code and test instances publicly available.  
- [ ] Describe tuning or parameter adjustment procedures clearly.  
- [ ] Include runtime calibration data.  
- [ ] Use consistent metrics across papers and implementations.

## ğŸš¨ Common Mistakes to Avoid
- [ ] Dominated or trivial algorithms.  
- [ ] Unstructured random testbeds.  
- [ ] One-run or â€œbest-onlyâ€ results.  
- [ ] Hand-tuned parameters without rules.  
- [ ] Deleting original data/code.  
- [ ] Ignoring unexplained anomalies.  
- [ ] Reporting non-reproducible timings.

---

âœ… **Follow these guidelines to transform experiments into real scientific contributions â€” not just code runs.**

---
